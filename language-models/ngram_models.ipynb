{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079e1d48",
   "metadata": {},
   "source": [
    "# N gramas\n",
    "The `NgramData` class, used to build language models with n-grams, is completely defined in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803f49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7f7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå This notebook assumes that corpus processing, tokenization and BoW construction was already performed on the notebook:\n",
    "# üëâ 'feature-extraction/bag_of_words.ipynb'\n",
    "\n",
    "#The variables used here (such as `BoW_tr`, `tr_txt`, `V1`, `dict_indices1`) were built there.\n",
    "#If you want to re-run the pipeline from scratch, check that file first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b7689",
   "metadata": {},
   "source": [
    "> üîó **Note:** The corpus loading, tokenization and construction of the Bag of Words is at\n",
    "> [`bag_of_words.ipynb`](./feature-extraction/bag_of_words.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76898656",
   "metadata": {},
   "source": [
    "# Ngram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b306637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N: int, vocab_max, tokenizer=None, embeddings_model=None):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.', '?', '!', ',', ';', ':', '^', '*', '+', '/', '\\\\', '\"', \"¬¥\", \"`\", \"¬®\", \"~\", \"{\", \"}\", \"[\", \"]\", \"(\", \")\", \"_\", \"-\", \"&\", \"%\", \"$\", \"#\", \"@\", \"¬ø\", \"?\", \"¬°\", \"!\", \"<\", \">\", \"=\", \"|\", \"¬∞\", \"¬¨\", \"¬¶\", \"¬™\", \"¬∫\", \"¬©\", \"¬Æ\", \"¬´\", \"¬ª\", \"‚Äú\", \"‚Äù\", \"‚Äò\", \"‚Äô\", \"‚Ä¶\", \"‚Äì\", \"‚Äî\", \"‚Ä¢\", \"¬∑\", \"¬ª\", \"¬´\", \"‚Ä¶\", \"‚Äò\", \"‚Äô\", \"‚Äú\", \"‚Äù\", \"‚Äì\", \"‚Äî\", \"‚Ä¢\", \"¬∑\", \"¬°\", \"¬ø\", \"<url>\", \"@usuario\", \"...\"])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = '<s>'\n",
    "        self.EOS = '</s>'\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.ngram_counts = Counter()\n",
    "        self.freq_counts = Counter()\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def default_tokenizer(self, doc: str) -> list:\n",
    "        return doc.split(\" \")\n",
    "\n",
    "    def remove_word(self, word: str) -> bool:\n",
    "        word = word.lower()\n",
    "        is_punct = word in self.punct\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    def get_vocabulary(self, corpus: list) -> set:\n",
    "        freq_dist = Counter([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not self.remove_word(w)])\n",
    "        sorted_words = [word for word, _ in freq_dist.most_common(self.vocab_max - 3)]\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    def fit(self, corpus: list) -> None:\n",
    "        self.vocab = self.get_vocabulary(corpus)\n",
    "        self.vocab.update({self.UNK, self.SOS, self.EOS})\n",
    "        \n",
    "        self.w2id = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.id2w = {i: word for word, i in self.w2id.items()}\n",
    "        \n",
    "        for doc in corpus:\n",
    "            for ngram in self.get_ngram_doc(doc):\n",
    "                self.ngram_counts[ngram] += 1\n",
    "        \n",
    "\n",
    "        self.freq_counts = Counter(self.ngram_counts.values())\n",
    "    \n",
    "    def transform(self, corpus: list) -> tuple:\n",
    "        X_ngrams, y = [], []\n",
    "        for doc in corpus:\n",
    "            for words_window in self.get_ngram_doc(doc):\n",
    "                words_window_ids = [self.w2id.get(w, self.w2id[self.UNK]) for w in words_window]\n",
    "                X_ngrams.append(words_window_ids[:-1])\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams), np.array(y)\n",
    "    \n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = [self.SOS] + self.replace_unk(self.tokenizer(doc)) + [self.EOS]\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def replace_unk(self, doc_tokens: list) -> list:\n",
    "        return [token if token.lower() in self.vocab else self.UNK for token in doc_tokens]\n",
    "    \n",
    " \n",
    "    def turing_smoothing(self, ngram: tuple) -> float:\n",
    "        if any(word not in self.vocab for word in ngram):\n",
    "            print(f\"El n-grama {ngram} contiene palabras fuera del vocabulario.\")\n",
    "            return 1 / sum(self.ngram_counts.values())  # Probabilidad m√≠nima\n",
    "    \n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        next_count = count + 1\n",
    "    \n",
    "        if count == 0:\n",
    "            return 1 / sum(self.ngram_counts.values())  \n",
    "        if count not in self.freq_counts or next_count not in self.freq_counts:\n",
    "            return count / sum(self.ngram_counts.values())\n",
    "  \n",
    "        if self.freq_counts[count] == 0:\n",
    "            return 1 / sum(self.ngram_counts.values())     \n",
    "        adjusted_count = (next_count * self.freq_counts[next_count]) / self.freq_counts[count]\n",
    "        return adjusted_count / sum(self.ngram_counts.values())\n",
    "    \n",
    "    \n",
    "    def turing_smoothing(self, ngram: tuple) -> float:\n",
    "        if any(word not in self.vocab for word in ngram):\n",
    "            return 1 / sum(self.ngram_counts.values())  \n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        next_count = count + 1\n",
    "        if count == 0:\n",
    "            return 1 / sum(self.ngram_counts.values())  \n",
    "        if next_count not in self.freq_counts:\n",
    "            adjusted_count = count\n",
    "        else:\n",
    "            adjusted_count = (next_count * self.freq_counts[next_count]) / self.freq_counts[count]\n",
    "        return adjusted_count / sum(self.ngram_counts.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b13204fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, N, lambda1=0.4, lambda2=0.3, lambda3=0.3, lambda4=0.0):\n",
    "        self.N = N  # (1 para unigramas, 2 para bigramas, etc.)\n",
    "        \n",
    "        self.lambda1 = lambda1 \n",
    "        self.lambda2 = lambda2  \n",
    "        self.lambda3 = lambda3  \n",
    "        self.lambda4 = lambda4  \n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.trigram_counts = Counter()\n",
    "        self.tetragram_counts = Counter()\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.total_tokens = 0\n",
    "        self.V = 0\n",
    "    \n",
    "    def train(self, transformed_corpus):\n",
    "        for tokens in transformed_corpus:\n",
    "            self.total_tokens += len(tokens)\n",
    "            for i, w in enumerate(tokens):\n",
    "                self.unigram_counts[w] += 1\n",
    "                if i > 0:\n",
    "                    w_prev = tokens[i - 1]\n",
    "                    self.bigram_counts[(w_prev, w)] += 1\n",
    "                if i > 1:\n",
    "                    w_prev2 = tokens[i - 2]\n",
    "                    self.trigram_counts[(w_prev2, w_prev, w)] += 1\n",
    "                if i > 2:\n",
    "                    w_prev3 = tokens[i - 3]\n",
    "                    self.tetragram_counts[(w_prev3, w_prev2, w_prev, w)] += 1\n",
    "        \n",
    "        self.vocab = set(self.unigram_counts.keys())\n",
    "        self.V = len(self.vocab)\n",
    "    \n",
    "    def mask_oov(self, word):\n",
    "        return \"<unk>\" if word not in self.vocab else word\n",
    "    \n",
    "    def unigram_prob(self, w):\n",
    "        return (self.unigram_counts.get(self.mask_oov(w), 0) + 1) / (self.total_tokens + self.V)\n",
    "    \n",
    "    def bigram_prob(self, w_prev, w):\n",
    "        return (self.bigram_counts.get((self.mask_oov(w_prev), self.mask_oov(w)), 0) + 1) / (self.unigram_counts.get(self.mask_oov(w_prev), 0) + self.V)\n",
    "    \n",
    "    def trigram_prob(self, w_prev2, w_prev, w):\n",
    "        return (self.trigram_counts.get((self.mask_oov(w_prev2), self.mask_oov(w_prev), self.mask_oov(w)), 0) + 1) / (self.bigram_counts.get((self.mask_oov(w_prev2), self.mask_oov(w_prev)), 0) + self.V)\n",
    "    \n",
    "    def tetragram_prob(self, w_prev3, w_prev2, w_prev, w):\n",
    "        return (self.tetragram_counts.get((self.mask_oov(w_prev3), self.mask_oov(w_prev2), self.mask_oov(w_prev), self.mask_oov(w)), 0) + 1) / (self.trigram_counts.get((self.mask_oov(w_prev3), self.mask_oov(w_prev2), self.mask_oov(w_prev)), 0) + self.V)\n",
    "    \n",
    "    def probability_of_word(self, context, w):\n",
    "        context = [self.mask_oov(w) for w in context]\n",
    "        if self.N == 1:\n",
    "            return self.unigram_prob(w)\n",
    "        elif self.N == 2:\n",
    "            return self.lambda3 * self.unigram_prob(w) + self.lambda2 * self.bigram_prob(context[0], w)\n",
    "        elif self.N == 3:\n",
    "            return self.lambda3 * self.unigram_prob(w) + self.lambda2 * self.bigram_prob(context[1], w) + self.lambda1 * self.trigram_prob(context[0], context[1], w)\n",
    "        elif self.N == 4:\n",
    "            return self.lambda4 * self.tetragram_prob(context[0], context[1], context[2], w) + self.lambda3 * self.unigram_prob(w) + self.lambda2 * self.bigram_prob(context[2], w) + self.lambda1 * self.trigram_prob(context[1], context[2], w)\n",
    "    \n",
    "    def sequence_probability(self, sequence):\n",
    "        prob = 1.0\n",
    "        for i in range(self.N - 1, len(sequence)):\n",
    "            context = sequence[i - self.N + 1:i]\n",
    "            w = sequence[i]\n",
    "            p = self.probability_of_word(context, w)\n",
    "            if p == 0:\n",
    "                return 0.0\n",
    "            prob *= p\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2376bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "N = 3  # trigramas\n",
    "vocab_max = 5000\n",
    "\n",
    "# Instancia y entrenamiento del extractor de n-gramas\n",
    "ngram_data = NgramData(N=N, vocab_max=vocab_max, tokenizer=lambda x: tokenizer.tokenize(x.lower()))\n",
    "ngram_data.fit(tr_txt)  # Aprende vocabulario y cuenta n-gramas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d572170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de entrada (IDs): [3116 3655]\n",
      "Palabras del n-grama: ['<s>', '<unk>']\n",
      "Palabra objetivo: <unk>\n"
     ]
    }
   ],
   "source": [
    "X, y = ngram_data.transform(tr_txt)\n",
    "\n",
    "print(\"Ejemplo de entrada (IDs):\", X[0])\n",
    "print(\"Palabras del n-grama:\", [ngram_data.id2w[i] for i in X[0]])\n",
    "print(\"Palabra objetivo:\", ngram_data.id2w[y[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05decce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todo el corpus a listas de tokens para el modelo\n",
    "corpus_tokenizado = [ngram_data.replace_unk(ngram_data.tokenizer(doc)) for doc in tr_txt]\n",
    "\n",
    "# Agregar <s> y </s> a cada oraci√≥n\n",
    "corpus_tokens = [[ngram_data.SOS] + tokens + [ngram_data.EOS] for tokens in corpus_tokenizado]\n",
    "\n",
    "# Entrenar el modelo\n",
    "lm = NgramLanguageModel(N=3)\n",
    "lm.train(corpus_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3c31b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de la secuencia: 0.0018864636553304092\n"
     ]
    }
   ],
   "source": [
    "# Probar probabilidad de una secuencia\n",
    "ejemplo = [\"odio\", \"a\", \"todos\"]\n",
    "ejemplo_ids = [w if w in ngram_data.vocab else \"<unk>\" for w in ejemplo]\n",
    "print(\"Probabilidad de la secuencia:\", lm.sequence_probability(ejemplo_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f3a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "571d493e",
   "metadata": {},
   "source": [
    "# N gram Models with $\\lambda$ interpolated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8aa0e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f280aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedNgramModel:\n",
    "    def __init__(self, lambda1=0.4, lambda2=0.3, lambda3=0.2, lambda4=0.1):\n",
    "        self.lambda1 = lambda1  \n",
    "        self.lambda2 = lambda2  \n",
    "        self.lambda3 = lambda3  \n",
    "        self.lambda4 = lambda4  \n",
    "        \n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.trigram_counts = Counter()\n",
    "        self.tetragram_counts = Counter()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "        self.V = 0\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        for tokens in corpus:\n",
    "            self.total_tokens += len(tokens)\n",
    "            for i, w in enumerate(tokens):\n",
    "                self.unigram_counts[w] += 1\n",
    "                if i > 0:\n",
    "                    self.bigram_counts[(tokens[i - 1], w)] += 1\n",
    "                if i > 1:\n",
    "                    self.trigram_counts[(tokens[i - 2], tokens[i - 1], w)] += 1\n",
    "                if i > 2:\n",
    "                    self.tetragram_counts[(tokens[i - 3], tokens[i - 2], tokens[i - 1], w)] += 1\n",
    "        \n",
    "        self.vocab = set(self.unigram_counts.keys())\n",
    "        self.V = len(self.vocab)\n",
    "    \n",
    "    def mask_oov(self, word):\n",
    "        return \"<unk>\" if word not in self.vocab else word\n",
    "    \n",
    "    def unigram_prob(self, w):\n",
    "        return (self.unigram_counts.get(self.mask_oov(w), 0) + 1) / (self.total_tokens + self.V)\n",
    "    \n",
    "    def bigram_prob(self, w_prev, w):\n",
    "        return (self.bigram_counts.get((self.mask_oov(w_prev), self.mask_oov(w)), 0) + 1) / (self.unigram_counts.get(self.mask_oov(w_prev), 0) + self.V)\n",
    "    \n",
    "    def trigram_prob(self, w_prev2, w_prev, w):\n",
    "        return (self.trigram_counts.get((self.mask_oov(w_prev2), self.mask_oov(w_prev), self.mask_oov(w)), 0) + 1) / (self.bigram_counts.get((self.mask_oov(w_prev2), self.mask_oov(w_prev)), 0) + self.V)\n",
    "    \n",
    "    def tetragram_prob(self, w_prev3, w_prev2, w_prev, w):\n",
    "        return (self.tetragram_counts.get((self.mask_oov(w_prev3), self.mask_oov(w_prev2), self.mask_oov(w_prev), self.mask_oov(w)), 0) + 1) / (self.trigram_counts.get((self.mask_oov(w_prev3), self.mask_oov(w_prev2), self.mask_oov(w_prev)), 0) + self.V)\n",
    "    \n",
    "    def interpolated_probability(self, context, w):\n",
    "        context = [self.mask_oov(w) for w in context]\n",
    "        return (self.lambda1 * self.tetragram_prob(*context[-3:], w) +\n",
    "                self.lambda2 * self.trigram_prob(*context[-2:], w) +\n",
    "                self.lambda3 * self.bigram_prob(*context[-1:], w) +\n",
    "                self.lambda4 * self.unigram_prob(w))\n",
    "    \n",
    "    def perplexity(self, corpus):\n",
    "        log_prob = 0\n",
    "        N = sum(len(sentence) for sentence in corpus)\n",
    "        for sentence in corpus:\n",
    "            for i in range(3, len(sentence)):\n",
    "                context = sentence[i - 3:i]\n",
    "                w = sentence[i]\n",
    "                prob = self.interpolated_probability(context, w)\n",
    "                log_prob += math.log(prob)\n",
    "        return math.exp(-log_prob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf9b70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(corpus):\n",
    "    random.shuffle(corpus)\n",
    "    train_size = int(0.8 * len(corpus))\n",
    "    val_size = int(0.1 * len(corpus))\n",
    "    train = corpus[:train_size]\n",
    "    val = corpus[train_size:train_size + val_size]\n",
    "    test = corpus[train_size + val_size:]\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86755ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "ngram_model = NgramData(N=4, vocab_max=5000, tokenizer=lambda x: tokenizer.tokenize(x.lower()))\n",
    "ngram_model.fit(tr_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c91c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las oraciones al formato con tokens especiales\n",
    "corpus = [[ngram_model.SOS] + ngram_model.replace_unk(ngram_model.tokenizer(doc)) + [ngram_model.EOS]\n",
    "          for doc in tr_txt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b71a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import math\n",
    "\n",
    "train_corpus, val_corpus, test_corpus = split_data(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "209e3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_model = InterpolatedNgramModel(lambda1=0.4, lambda2=0.3, lambda3=0.2, lambda4=0.1)\n",
    "inter_model.train(train_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70c9c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidad en validaci√≥n: 216.37\n",
      "Perplejidad en prueba: 204.77\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplejidad en validaci√≥n: {inter_model.perplexity(val_corpus):.2f}\")\n",
    "print(f\"Perplejidad en prueba: {inter_model.perplexity(test_corpus):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb86070",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

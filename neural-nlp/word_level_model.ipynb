{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word models\n",
    "\n",
    "This code uses a special kind of neural network called an \"N-gram-based neural language model\" to do its work. It uses a tool called PyTorch to do this. It has custom or pre-trained embeddings.\n",
    "Classical training with early stopping.\n",
    "This tool can be used to compare how similar two words are and how precise they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple #Tiping es una libreria que permite definir tipos de variables\n",
    "from argparse import Namespace #Permite definir argumentos de entrada\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Prepocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import math\n",
    "\n",
    "\n",
    "#Pythorch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_recall_fscore_support\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2 #esta se tiene que cambiar por una a mano\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "#Importar librerias de sklearn\n",
    "from sklearn import svm \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1111\n",
    "random.seed(seed) #phyton seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed) #torch seed\n",
    "torch.backends.cudnn.deterministic = False #Se refiere a la implementacion de cudnn, que es la libreria de pytorch que permite hacer calculos en la GPU, False para que no sea determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('mex20_train.txt',sep='\\r\\n',engine='python',header=None).loc[:,0].values.tolist()\n",
    "X_val=pd.read_csv('mex20_val.txt',sep='\\r\\n',engine='python',header=None).loc[:,0].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Namespace()\n",
    "args.N=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N:int,vocab_max:int=5000,tokenizer=None, embeddings_model=None):\n",
    "        self.tokenizer=tokenizer if tokenizer else self.default_tokenizer()\n",
    "        self.punct=set(['.','?','!',',',';',':','^','*','+','/','\\\\','\"','´','`','¨','~','{','}','[',']','(',')','_','-','&','%','$','#','@','¿','?','¡','!','<','>','=','|','°','¬','¦','ª','º','©','®','«','»','“','”','‘','’','…','–','—','•','·','»','«','…','‘','’','“','”','–','—','•','·','¡','¿','<url>','@usuario','...'])\n",
    "        self.N=N\n",
    "        self.vocab_max=vocab_max\n",
    "        self.UNK=\"<unk>\"\n",
    "        self.SOS='<s>'\n",
    "        self.EOS='</s>'\n",
    "        self.embeddings_model=embeddings_model\n",
    "        \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def default_tokenizer(self,doc:str) -> list: #-> es para definir el tipo de variable que regresa\n",
    "        return doc.split(\" \")\n",
    "    \n",
    "    \n",
    "    def remove_word(self,word:str) -> bool:\n",
    "        word=word.lower()\n",
    "        is_punct=True if word in self.punct else False\n",
    "        is_digit=word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    def get_vocabulary(self,corpus:list) -> set:\n",
    "        freq_dist=FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not  self.remove_word(w)]) #genera un diccionario de palabras y su frecuencia\n",
    "        sorted_words=self.sortFreqDict(freq_dist)[:self.vocab_max-3] #selecciona las palabras mas frecuentes del diccionario creado, el -3 es para dejar espacio para los tokens especiales\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    def sortFreqDict(self,freq_dist) -> list:\n",
    "        freq_dict=dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "    \n",
    "    \n",
    " \n",
    "        \n",
    "    def fit(self,corpus:list) -> None:\n",
    "        self.vocab=self.get_vocabulary(corpus)#tokens especiales\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "        \n",
    "        #mapeo id a palabra y viceversa\n",
    "        self.w2id={}\n",
    "        self.id2w={}\n",
    "        #embeddings preentrenados\n",
    "        #vocab = sorted(self.vocab)  #\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embeddings_matrix=np.empty((len(self.vocab),self.embeddings_model.vector_size))\n",
    "            \n",
    "        id=0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_=word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_]=id\n",
    "                    self.id2w[id]=word_\n",
    "                    #solo crear la sub matriz de los que importan\n",
    "                    \n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word_ in self.embeddings_model:\n",
    "                            self.embeddings_matrix[id]=self.embeddings_model[word_]\n",
    "                        else:\n",
    "                            self.embeddings_matrix[id]=np.random.rand(self.embeddings_model.vector_size)\n",
    "                    id+=1\n",
    "        #siempre agregar los tokens especiales  \n",
    "        self.w2id.update({self.UNK:id,self.SOS:id+1,self.EOS:id+2})\n",
    "        self.id2w.update({id:self.UNK,id+1:self.SOS,id+2:self.EOS})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def transform(self,corpus:list)-> Tuple [np.array,np.array]:\n",
    "        X_ngrams=[]\n",
    "        y=[]\n",
    "        for doc in corpus:\n",
    "            doc_ngram=self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                #print(\"words_window:\", words_window)\n",
    "                words_window_ids = [self.w2id[w] if w in self.w2id else self.w2id[\"<unk>\"] for w in words_window]\n",
    "\n",
    "                #words_window_ids=[self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams),np.array(y)\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]  # Corrección aquí\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def replace_unk(self,doc_tokens:list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab: #si el token no essta en minuscula entonces reemplazar por UNK\n",
    "                doc_tokens[i]=self.UNK\n",
    "        return doc_tokens #regresa tokens ya procesados\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk=TweetTokenizer()\n",
    "ngram_data=NgramData(args.N,5000,tk.tokenize)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train,y_ngram_train=ngram_data.transform(X_train)\n",
    "X_ngram_val,y_ngram_val=ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations (X): (102751, 3),y: (102751,)\n",
      "Validation observations (X): (11558, 3),y: (11558,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training observations (X): {X_ngram_train.shape},y: {y_ngram_train.shape}')\n",
    "print(f'Validation observations (X): {X_ngram_val.shape},y: {y_ngram_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set batch size in args\n",
    "args.batch_size=64\n",
    "#Num workers \n",
    "args.num_workers=0\n",
    "\n",
    "#Train\n",
    "train_dataset=TensorDataset(torch.tensor(X_ngram_train,dtype=torch.int64),torch.tensor(y_ngram_train,dtype=torch.int64))\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "\n",
    "#Val\n",
    "val_dataset=TensorDataset(torch.tensor(X_ngram_val,dtype=torch.int64),torch.tensor(y_ngram_val,dtype=torch.int64))\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocab size\n",
    "args.vocab_size=ngram_data.get_vocab_size()\n",
    "\n",
    "#dimension of word embeddings\n",
    "args.d=50\n",
    "args.d_h=100\n",
    "args.dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self,args, pretrained_embeddings=None):\n",
    "        super(NeuralLM,self).__init__()\n",
    "        self.window_size=args.N-1\n",
    "        self.embedding_dim=args.d\n",
    "        \n",
    "        #self.emb = nn.Embedding.from_pretrained(\n",
    "        #    torch.tensor(pretrained_embeddings, dtype=torch.float32),\n",
    "        #    freeze=False  # Debe ser False para que los embeddings se actualicen\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        # Si hay embeddings preentrenados, los usamos\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.emb = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(pretrained_embeddings, dtype=torch.float32),\n",
    "            freeze=False  # Debe ser False para que los embeddings se actualicen\n",
    "            )\n",
    "        #    self.emb = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings, dtype=torch.float32), freeze=False)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1=nn.Linear(args.d*(args.N-1),args.d_h)\n",
    "        #self.fc1=nn.Linear(args.d * self.window_size, args.d_h)\n",
    "        self.drop1=nn.Dropout(p=args.dropout)\n",
    "        self.fc2=nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizar la red para obtener la probabilidad 1\n",
    "def get_preds(raw_logits):\n",
    "    probs=F.softmax(raw_logits.detach(),dim=1) #solamente a los valores de la matriz del tensor\n",
    "    y_pred=torch.argmax(probs,dim=1).cpu().numpy() #argmax para indice\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data,model,gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds,tgts=[],[]\n",
    "        for window_words,labels in data:\n",
    "            if gpu:\n",
    "                window_words=window_words.cuda()\n",
    "            outputs=model(window_words)\n",
    "            \n",
    "            #Get predictions\n",
    "            y_pred=get_preds(outputs)\n",
    "            tgt=labels.numpy()\n",
    "            tgt_list = tgt.tolist()  # Convertir a lista de Python\n",
    "            tgts.append(tgt_list)  # Ahora sí puedes hacer append()\n",
    "\n",
    "            #tgt.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts=[e for l in tgts for e in l]\n",
    "    preds=[e for l in preds for e in l]\n",
    "    \n",
    "    return accuracy_score(tgts,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state,is_best,checkpoint_path,filename='checkpoint.pt'):\n",
    "    filename=os.path.join(checkpoint_path,filename)\n",
    "    torch.save(state,filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename,os.path.join(checkpoint_path,'model_best.pt')) #shutil es una libreria que permite copiar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"word2vec_col.txt\"\n",
    "word_to_idx = {}\n",
    "embeddings_list = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = f.readline().strip().split()\n",
    "    vocab_size = int(first_line[0])  \n",
    "    emb_dim = int(first_line[1])  \n",
    "\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]  \n",
    "        vector = np.array(parts[1:], dtype=np.float32) \n",
    "        word_to_idx[word] = len(word_to_idx)  \n",
    "        embeddings_list.append(vector)\n",
    "\n",
    "embedding_matrix_pre = np.vstack(embeddings_list)\n",
    "\n",
    "vocab_size, emb_dim, embedding_matrix_pre.shape\n",
    "\n",
    "pretrained_embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix_pre, dtype=torch.float32), freeze=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "args.vocab_size=ngram_data.get_vocab_size()\n",
    "args.d=100 #dimension of word embeddings\n",
    "args.d_h=200 #dimension of hidden layer\n",
    "args.dropout=0.1\n",
    "\n",
    "#Training hyperparameters\n",
    "args.lr=2.3e-1\n",
    "args.num_epochs=100\n",
    "args.patience=20 #si despues de 20 epocas no mejora el modelo entonces se detiene\n",
    "\n",
    "#scheduler hyperparameters\n",
    "args.lr_patience=10\n",
    "args.lr_factor=0.5\n",
    "\n",
    "#Saving directory\n",
    "args.save_dir='model'\n",
    "os.makedirs(args.save_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "## 1.1 Embeddings pre entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con embeddings preentrenados\n",
    "model1 = NeuralLM(args, pretrained_embeddings=embedding_matrix_pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 0\n",
    "n_no_improve = 0  \n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model1 = model1.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=args.lr_factor, patience=args.lr_patience, verbose=True)\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 5.7847, Val Acc: 0.1168, Train Acc: 0.1453, Time: 54.40s\n",
      "Epoch [2/100], Loss: 5.3253, Val Acc: 0.2020, Train Acc: 0.1641, Time: 55.19s\n",
      "Epoch [3/100], Loss: 5.1008, Val Acc: 0.2041, Train Acc: 0.1700, Time: 52.82s\n",
      "Epoch [4/100], Loss: 4.9338, Val Acc: 0.1473, Train Acc: 0.1721, Time: 52.32s\n",
      "Epoch [5/100], Loss: 4.7812, Val Acc: 0.1726, Train Acc: 0.1757, Time: 52.22s\n",
      "Epoch [6/100], Loss: 4.6538, Val Acc: 0.2174, Train Acc: 0.1773, Time: 57.83s\n",
      "Epoch [7/100], Loss: 4.5293, Val Acc: 0.1844, Train Acc: 0.1813, Time: 58.72s\n",
      "Epoch [8/100], Loss: 4.4258, Val Acc: 0.1492, Train Acc: 0.1830, Time: 64.94s\n",
      "Epoch [9/100], Loss: 4.3332, Val Acc: 0.1602, Train Acc: 0.1880, Time: 63.49s\n",
      "Epoch [10/100], Loss: 4.2494, Val Acc: 0.1760, Train Acc: 0.1918, Time: 58.34s\n",
      "Epoch [11/100], Loss: 4.1723, Val Acc: 0.1559, Train Acc: 0.1954, Time: 55.12s\n",
      "Epoch [12/100], Loss: 4.1005, Val Acc: 0.1602, Train Acc: 0.2007, Time: 56.77s\n",
      "Epoch [13/100], Loss: 3.6360, Val Acc: 0.1874, Train Acc: 0.2448, Time: 54.86s\n",
      "Epoch [14/100], Loss: 3.5419, Val Acc: 0.2082, Train Acc: 0.2508, Time: 58.45s\n",
      "Epoch [15/100], Loss: 3.4987, Val Acc: 0.1975, Train Acc: 0.2579, Time: 57.00s\n",
      "Epoch [16/100], Loss: 3.4622, Val Acc: 0.1807, Train Acc: 0.2598, Time: 58.61s\n",
      "Epoch [17/100], Loss: 3.4286, Val Acc: 0.1762, Train Acc: 0.2637, Time: 114.69s\n",
      "Epoch [18/100], Loss: 3.3989, Val Acc: 0.2055, Train Acc: 0.2667, Time: 135.86s\n",
      "Epoch [19/100], Loss: 3.3689, Val Acc: 0.1978, Train Acc: 0.2723, Time: 149.18s\n",
      "Epoch [20/100], Loss: 3.3471, Val Acc: 0.1686, Train Acc: 0.2750, Time: 143.07s\n",
      "Epoch [21/100], Loss: 3.3207, Val Acc: 0.2246, Train Acc: 0.2763, Time: 123.70s\n",
      "Epoch [22/100], Loss: 3.2982, Val Acc: 0.1992, Train Acc: 0.2812, Time: 118.61s\n",
      "Epoch [23/100], Loss: 3.2770, Val Acc: 0.1963, Train Acc: 0.2844, Time: 60.70s\n",
      "Epoch [24/100], Loss: 3.0387, Val Acc: 0.2051, Train Acc: 0.3190, Time: 57.77s\n",
      "Epoch [25/100], Loss: 3.0065, Val Acc: 0.1958, Train Acc: 0.3234, Time: 60.07s\n",
      "Epoch [26/100], Loss: 2.9845, Val Acc: 0.2005, Train Acc: 0.3279, Time: 67.86s\n",
      "Epoch [27/100], Loss: 2.9763, Val Acc: 0.1890, Train Acc: 0.3274, Time: 58.48s\n",
      "Epoch [28/100], Loss: 2.9651, Val Acc: 0.1884, Train Acc: 0.3281, Time: 62.15s\n",
      "Epoch [29/100], Loss: 2.9550, Val Acc: 0.1955, Train Acc: 0.3284, Time: 68.73s\n",
      "Epoch [30/100], Loss: 2.9439, Val Acc: 0.1938, Train Acc: 0.3314, Time: 76.26s\n",
      "Epoch [31/100], Loss: 2.9367, Val Acc: 0.2153, Train Acc: 0.3312, Time: 63.42s\n",
      "Epoch [32/100], Loss: 2.9256, Val Acc: 0.2139, Train Acc: 0.3324, Time: 67.82s\n",
      "Epoch [33/100], Loss: 2.9182, Val Acc: 0.2006, Train Acc: 0.3343, Time: 58.95s\n",
      "Epoch [34/100], Loss: 2.9075, Val Acc: 0.2002, Train Acc: 0.3363, Time: 56.78s\n",
      "Epoch [35/100], Loss: 2.7875, Val Acc: 0.2044, Train Acc: 0.3588, Time: 58.14s\n",
      "Epoch [36/100], Loss: 2.7704, Val Acc: 0.2099, Train Acc: 0.3605, Time: 58.74s\n",
      "Epoch [37/100], Loss: 2.7612, Val Acc: 0.2075, Train Acc: 0.3622, Time: 67.40s\n",
      "Epoch [38/100], Loss: 2.7567, Val Acc: 0.2163, Train Acc: 0.3626, Time: 58.43s\n",
      "Epoch [39/100], Loss: 2.7542, Val Acc: 0.2106, Train Acc: 0.3632, Time: 64.51s\n",
      "Epoch [40/100], Loss: 2.7474, Val Acc: 0.2112, Train Acc: 0.3637, Time: 58.68s\n",
      "No improvement. Breaking training loop.\n",
      "--- Training completed in 2874.9617490768433 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model1.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words, labels = window_words.cuda(), labels.cuda()\n",
    "            \n",
    "        outputs = model1(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "    \n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    model1.eval()\n",
    "    tuning_metric = model_eval(val_loader, model1, gpu=args.use_gpu)\n",
    "    metric_history.append(tuning_metric)  \n",
    "\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "    \n",
    "    save_checkpoint({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"state_dict\": model1.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "    }, is_improvement, args.save_dir)\n",
    "    \n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking training loop.\")\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{args.num_epochs}], Loss: {np.mean(loss_epoch):.4f}, '\n",
    "          f'Val Acc: {tuning_metric:.4f}, Train Acc: {mean_epoch_metric:.4f}, '\n",
    "          f'Time: {time.time() - epoch_start_time:.2f}s')\n",
    "\n",
    "print(\"--- Training completed in %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3r/fzd1rrd908j_nt0tr8b5tkfh0000gn/T/ipykernel_6831/97214880.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('model/model_best.pt', map_location=torch.device('cpu'))  # Cargar en CPU por defecto\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(973265, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "best_model_pre = NeuralLM(args, pretrained_embeddings=embedding_matrix_pre)\n",
    "checkpoint = torch.load('model/model_best.pt', map_location=torch.device('cpu'))  \n",
    "best_model_pre.load_state_dict(checkpoint['state_dict'])\n",
    "best_model_pre.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embedding, ngram_data, word, n):\n",
    "    if word not in ngram_data.w2id:\n",
    "        print(f\"La palabra '{word}' no está en el vocabulario.\")\n",
    "        return\n",
    "    \n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embedding(word_id)\n",
    "    dists = torch.norm(embedding.weight - word_embed, dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1])\n",
    "\n",
    "    print(f\"{'-'*25} Palabras más similares a '{word}' {'-'*25}\")\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        word_closest = ngram_data.id2w.get(idx, \"<unk>\")  # Usa \"<unk>\" si no encuentra la palabra\n",
    "        print(word_closest, difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra 'vez' está en el vocabulario.\n",
      "------------------------- Palabras más similares a 'vez' -------------------------\n",
      "😪 18.096907\n",
      "dejan 21.678274\n",
      "<unk> 23.59277\n",
      "<unk> 24.248632\n",
      "tuyo 24.355232\n",
      "agresivo 24.852814\n",
      "votando 25.206497\n",
      "irán 25.450642\n",
      "<unk> 26.123274\n",
      "<unk> 26.426844\n"
     ]
    }
   ],
   "source": [
    "word = 'vez'  \n",
    "\n",
    "if word in ngram_data.w2id:\n",
    "    print(f\"La palabra '{word}' está en el vocabulario.\")\n",
    "    print_closest_words(best_model_pre.emb, ngram_data, word, 10)\n",
    "else:\n",
    "    print(f\"La palabra '{word}' NO está en el vocabulario.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text,tokenizer):\n",
    "    all_tokens=[w.lower() if w in ngram_data.w2id else '<unk>' for w in tokenizer.tokenize(text)] #verificar que este en el diccionario\n",
    "    tokens_ids=[ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens,tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits,temperature=1.0):\n",
    "    \n",
    "    logits=np.array(logits).astype('float64')\n",
    "    \n",
    "    preds=logits/temperature\n",
    "    exp_preds=np.exp(preds)\n",
    "    preds=exp_preds/np.sum(exp_preds)\n",
    "    \n",
    "    probas=np.random.multinomial(1,preds)\n",
    "    return np.argmax(probas)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model,token_ids):\n",
    "    words_ids_tensor=torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred=model(words_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    \n",
    "    y_pred=sample_next_word(y_raw_pred,1.0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model,initial_text,tokenizer): #entrada de 3 tokens\n",
    "    all_tokens,window_word_ids=parse_text(initial_text,tokenizer)\n",
    "    for i in range(100):\n",
    "        y_pred=predict_next_token(model,window_word_ids) #numero\n",
    "        next_word=ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        \n",
    "        if next_word == '<\\s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    return \" \".join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Secuencia generada\n",
      "------------------------------\n",
      "me importa tu dentro cobijas decir aaaaaah perdonar app responsabilidad llevamos vídeos bailar 🎵 pronta io sabemos sacarle hacerle europa ruco reputisima técnico ronchas bastante pájaros limpia trabaja victor antojos capaz serían quedé supongo zaragoza 🤦🏾‍♂ máquina trastornado quedas instinto esposo samudio da iletradas sacarse tiempo 👅 ine ideología mostraste llegue putazos banca trabajan lamemelapanucha chilenos spotify mia tarjetas comido dignidad policías #una hahaha fachas curioso solo reputa tolerar baja rodillas jajajajajajajajajaja #ruggeropasquarelli izquierdos bebe tecates tonta word cambiando fascina modelos cejas trump joto chamba saber osorio circulan pega contrario love iba hola 👍 valí pelar ni ardida situación lee 👌🏼 pasará ¯\n"
     ]
    }
   ],
   "source": [
    "initial_tokens= \" Me importa tu \"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Secuencia generada\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model_pre,initial_tokens,tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likehood(model,text,ngram_model):\n",
    "    X,y=ngram_data.transform([text])\n",
    "    X,y=X[2:],y[2:]\n",
    "    X=torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    logits=model(X).detach()\n",
    "    probs=F.softmax(logits,dim=1).numpy()\n",
    "    \n",
    "    return (np.sum([np.log(probs[i][w]) for i,w in enumerate(y)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Las más probables\n",
      "-19.440208 natural estamos en la clase de lenguaje\n",
      "-20.465725 estamos natural en la clase de lenguaje\n",
      "-23.164532 estamos natural de clase en la lenguaje\n",
      "-24.567703 natural estamos en la de clase lenguaje\n",
      "-24.582191 natural estamos de clase en la lenguaje\n",
      "------------------------------\n",
      "Las menos probables\n",
      "-83.05227 en de lenguaje clase estamos natural la\n",
      "-84.606544 lenguaje en de clase estamos natural la\n",
      "-85.820175 lenguaje de natural estamos clase la en\n",
      "-86.08365 de lenguaje estamos natural clase la en\n",
      "-89.090355 de lenguaje clase estamos natural la en\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list=\"estamos en la clase de lenguaje natural\".split(' ')\n",
    "perms=[' '.join(perm) for perm in permutations(word_list)]\n",
    "print(\"-\" * 30)\n",
    "print(\"Las más probables\")\n",
    "\n",
    "for p,t in sorted([(log_likehood(best_model_pre,text,ngram_data),text) for text in perms], reverse=True)[:5]:\n",
    "    print(p,t)\n",
    "    \n",
    "print(\"-\" * 30)\n",
    "print(\"Las menos probables\")\n",
    "\n",
    "for p,t in sorted([(log_likehood(best_model_pre,text,ngram_data),text) for text in perms], reverse=True)[-5:]:\n",
    "    print(p,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coca cola es': -19.389812,\n",
       " 'me gusta el': -14.146886,\n",
       " 'me trata mejor': -18.892828,\n",
       " 'quiero unos tacos': -12.596671,\n",
       " 'cafe con leche': -15.369717}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"coca cola es\",\n",
    "    \"me gusta el\",\n",
    "    \"me trata mejor\",\n",
    "    \"quiero unos tacos\",\n",
    "    \"cafe con leche\"\n",
    "]\n",
    "\n",
    "likelihoods = {sentence: log_likehood(best_model_pre, sentence, ngram_data) for sentence in sentences}\n",
    "likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con embeddings preentrenados 256.64211944881106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perplexity(model, data_loader, ngram_data, use_gpu=False):\n",
    "    total_log_likelihood = 0\n",
    "    total_word_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for window_words, labels in data_loader:\n",
    "            if use_gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            outputs = model(window_words)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "\n",
    "            batch_log_likelihood = log_probs[range(labels.shape[0]), labels].sum().item()\n",
    "            total_log_likelihood += batch_log_likelihood\n",
    "            total_word_count += labels.shape[0]\n",
    "\n",
    "    avg_log_likelihood = total_log_likelihood / total_word_count\n",
    "    perplexity = math.exp(-avg_log_likelihood)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "perplexity_pretrained = perplexity(best_model_pre, val_loader, ngram_data, use_gpu=args.use_gpu)\n",
    "print(\"Modelo con embeddings preentrenados\", perplexity_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## 1.2 Modelo sin embeddings pre entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreasolorio/Library/Python/3.9/lib/python/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 5.4366, Val Acc: 0.2342, Train Acc: 0.1872, Time: 5.49s\n",
      "Epoch [2/100], Loss: 5.0023, Val Acc: 0.1765, Train Acc: 0.1976, Time: 4.85s\n",
      "Epoch [3/100], Loss: 4.7895, Val Acc: 0.2211, Train Acc: 0.2019, Time: 4.68s\n",
      "Epoch [4/100], Loss: 4.6274, Val Acc: 0.2319, Train Acc: 0.2065, Time: 4.59s\n",
      "Epoch [5/100], Loss: 4.4834, Val Acc: 0.2229, Train Acc: 0.2095, Time: 4.63s\n",
      "Epoch [6/100], Loss: 4.3494, Val Acc: 0.2256, Train Acc: 0.2123, Time: 4.51s\n",
      "Epoch [7/100], Loss: 4.2340, Val Acc: 0.2099, Train Acc: 0.2151, Time: 4.59s\n",
      "Epoch [8/100], Loss: 4.1221, Val Acc: 0.2272, Train Acc: 0.2165, Time: 4.51s\n",
      "Epoch [9/100], Loss: 4.0126, Val Acc: 0.1589, Train Acc: 0.2195, Time: 4.56s\n",
      "Epoch [10/100], Loss: 3.9101, Val Acc: 0.1589, Train Acc: 0.2237, Time: 4.54s\n",
      "Epoch [11/100], Loss: 3.8195, Val Acc: 0.1907, Train Acc: 0.2265, Time: 4.69s\n",
      "Epoch [12/100], Loss: 3.7332, Val Acc: 0.1761, Train Acc: 0.2310, Time: 5.77s\n",
      "Epoch [13/100], Loss: 3.6453, Val Acc: 0.2293, Train Acc: 0.2408, Time: 6.17s\n",
      "Epoch [14/100], Loss: 3.5756, Val Acc: 0.2374, Train Acc: 0.2475, Time: 5.57s\n",
      "Epoch [15/100], Loss: 3.4971, Val Acc: 0.1847, Train Acc: 0.2560, Time: 5.26s\n",
      "Epoch [16/100], Loss: 3.4429, Val Acc: 0.2049, Train Acc: 0.2630, Time: 6.05s\n",
      "Epoch [17/100], Loss: 3.3679, Val Acc: 0.1916, Train Acc: 0.2714, Time: 5.61s\n",
      "Epoch [18/100], Loss: 3.3146, Val Acc: 0.1562, Train Acc: 0.2803, Time: 5.23s\n",
      "Epoch [19/100], Loss: 3.2610, Val Acc: 0.1066, Train Acc: 0.2880, Time: 5.65s\n",
      "Epoch [20/100], Loss: 3.2247, Val Acc: 0.2387, Train Acc: 0.2925, Time: 5.48s\n",
      "Epoch [21/100], Loss: 3.1821, Val Acc: 0.1353, Train Acc: 0.2998, Time: 5.19s\n",
      "Epoch [22/100], Loss: 3.1367, Val Acc: 0.1371, Train Acc: 0.3058, Time: 5.01s\n",
      "Epoch [23/100], Loss: 3.1012, Val Acc: 0.2287, Train Acc: 0.3119, Time: 5.28s\n",
      "Epoch [24/100], Loss: 3.0647, Val Acc: 0.1623, Train Acc: 0.3170, Time: 5.35s\n",
      "Epoch [25/100], Loss: 3.0360, Val Acc: 0.1236, Train Acc: 0.3212, Time: 5.24s\n",
      "Epoch [26/100], Loss: 3.0020, Val Acc: 0.2261, Train Acc: 0.3261, Time: 5.13s\n",
      "Epoch [27/100], Loss: 2.9730, Val Acc: 0.2316, Train Acc: 0.3305, Time: 5.19s\n",
      "Epoch [28/100], Loss: 2.9498, Val Acc: 0.2305, Train Acc: 0.3342, Time: 5.42s\n",
      "Epoch [29/100], Loss: 2.9247, Val Acc: 0.2137, Train Acc: 0.3392, Time: 5.19s\n",
      "Epoch [30/100], Loss: 2.8979, Val Acc: 0.2080, Train Acc: 0.3434, Time: 4.79s\n",
      "Epoch [31/100], Loss: 2.6228, Val Acc: 0.1898, Train Acc: 0.3897, Time: 5.31s\n",
      "Epoch [32/100], Loss: 2.5823, Val Acc: 0.2223, Train Acc: 0.3943, Time: 5.03s\n",
      "Epoch [33/100], Loss: 2.5630, Val Acc: 0.1851, Train Acc: 0.3976, Time: 5.46s\n",
      "Epoch [34/100], Loss: 2.5570, Val Acc: 0.2270, Train Acc: 0.3973, Time: 4.89s\n",
      "Epoch [35/100], Loss: 2.5467, Val Acc: 0.2137, Train Acc: 0.3994, Time: 5.84s\n",
      "Epoch [36/100], Loss: 2.5371, Val Acc: 0.1879, Train Acc: 0.4007, Time: 5.46s\n",
      "Epoch [37/100], Loss: 2.5258, Val Acc: 0.2065, Train Acc: 0.4017, Time: 5.48s\n",
      "Epoch [38/100], Loss: 2.5213, Val Acc: 0.1555, Train Acc: 0.4018, Time: 5.62s\n",
      "Epoch [39/100], Loss: 2.5088, Val Acc: 0.1980, Train Acc: 0.4037, Time: 4.56s\n",
      "No improvement. Breaking training loop.\n",
      "--- Training completed in 206.3864541053772 seconds ---\n"
     ]
    }
   ],
   "source": [
    "model_no_pre = NeuralLM(args)  # Modelo sin embeddings preentrenados\n",
    "torch.save(model_no_pre.state_dict(), 'model/model_no_embeddings.pt')\n",
    "best_metric = 0\n",
    "n_no_improve = 0  \n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model_no_pre = model_no_pre.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_no_pre.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=args.lr_factor, patience=args.lr_patience, verbose=True)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model_no_pre.train()\n",
    "    \n",
    "    for window_words, labels in train_loader:\n",
    "        if args.use_gpu:\n",
    "            window_words, labels = window_words.cuda(), labels.cuda()\n",
    "            \n",
    "        outputs = model_no_pre(window_words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt, y_pred))\n",
    "    \n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "\n",
    "    model_no_pre.eval()\n",
    "    tuning_metric = model_eval(val_loader, model_no_pre, gpu=args.use_gpu)\n",
    "    metric_history.append(tuning_metric)  \n",
    "\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "    \n",
    "    save_checkpoint({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"state_dict\": model_no_pre.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "    }, is_improvement, args.save_dir)\n",
    "    \n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking training loop.\")\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{args.num_epochs}], Loss: {np.mean(loss_epoch):.4f}, '\n",
    "          f'Val Acc: {tuning_metric:.4f}, Train Acc: {mean_epoch_metric:.4f}, '\n",
    "          f'Time: {time.time() - epoch_start_time:.2f}s')\n",
    "\n",
    "print(\"--- Training completed in %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_no_pre = NeuralLM(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

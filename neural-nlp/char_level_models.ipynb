{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char level models\n",
    "\n",
    "This code uses a special kind of neural network called an \"N-gram-based neural language model\" to do its work. It uses a tool called PyTorch to do this. It has custom or pre-trained embeddings.\n",
    "Classical training with early stopping.\n",
    "This tool can be used to compare how similar two chars are and how precise they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tools\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple #Tiping es una libreria que permite definir tipos de variables\n",
    "from argparse import Namespace #Permite definir argumentos de entrada\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Prepocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import math\n",
    "\n",
    "\n",
    "#Pythorch\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_recall_fscore_support\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2 #esta se tiene que cambiar por una a mano\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "#Importar librerias de sklearn\n",
    "from sklearn import svm \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics \n",
    "\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1111\n",
    "random.seed(seed) #phyton seed\n",
    "np.random.seed(seed) #numpy seed\n",
    "torch.manual_seed(seed) #torch seed\n",
    "torch.backends.cudnn.deterministic = False #Se refiere a la implementacion de cudnn, que es la libreria de pytorch que permite hacer calculos en la GPU, False para que no sea determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('mex20_train.txt',sep='\\r\\n',engine='python',header=None).loc[:,0].values.tolist()\n",
    "X_val=pd.read_csv('mex20_val.txt',sep='\\r\\n',engine='python',header=None).loc[:,0].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Namespace()\n",
    "args.N=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    def __init__(self, N:int,vocab_max:int=5000,tokenizer=None, embeddings_model=None):\n",
    "        self.tokenizer=tokenizer if tokenizer else self.default_tokenizer()\n",
    "        self.punct=set(['.','?','!',',',';',':','^','*','+','/','\\\\','\"','´','`','¨','~','{','}','[',']','(',')','_','-','&','%','$','#','@','¿','?','¡','!','<','>','=','|','°','¬','¦','ª','º','©','®','«','»','“','”','‘','’','…','–','—','•','·','»','«','…','‘','’','“','”','–','—','•','·','¡','¿','<url>','@usuario','...'])\n",
    "        self.N=N\n",
    "        self.vocab_max=vocab_max\n",
    "        self.UNK=\"<unk>\"\n",
    "        self.SOS='<s>'\n",
    "        self.EOS='</s>'\n",
    "        self.embeddings_model=embeddings_model\n",
    "        \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def default_tokenizer(self,doc:str) -> list: #-> es para definir el tipo de variable que regresa\n",
    "        return doc.split(\" \")\n",
    "    \n",
    "    \n",
    "    def remove_word(self,word:str) -> bool:\n",
    "        word=word.lower()\n",
    "        is_punct=True if word in self.punct else False\n",
    "        is_digit=word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    def get_vocabulary(self,corpus:list) -> set:\n",
    "        freq_dist=FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not  self.remove_word(w)]) #genera un diccionario de palabras y su frecuencia\n",
    "        sorted_words=self.sortFreqDict(freq_dist)[:self.vocab_max-3] #selecciona las palabras mas frecuentes del diccionario creado, el -3 es para dejar espacio para los tokens especiales\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    def sortFreqDict(self,freq_dist) -> list:\n",
    "        freq_dict=dict(freq_dist)\n",
    "        return sorted(freq_dict, key=freq_dict.get, reverse=True)\n",
    "    \n",
    "    \n",
    " \n",
    "        \n",
    "    def fit(self,corpus:list) -> None:\n",
    "        self.vocab=self.get_vocabulary(corpus)#tokens especiales\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "        \n",
    "        #mapeo id a palabra y viceversa\n",
    "        self.w2id={}\n",
    "        self.id2w={}\n",
    "        #embeddings preentrenados\n",
    "        #vocab = sorted(self.vocab)  #\n",
    "        if self.embeddings_model is not None:\n",
    "            self.embeddings_matrix=np.empty((len(self.vocab),self.embeddings_model.vector_size))\n",
    "            \n",
    "        id=0\n",
    "        for doc in corpus:\n",
    "            for word in self.tokenizer(doc):\n",
    "                word_=word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_]=id\n",
    "                    self.id2w[id]=word_\n",
    "                    #solo crear la sub matriz de los que importan\n",
    "                    \n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word_ in self.embeddings_model:\n",
    "                            self.embeddings_matrix[id]=self.embeddings_model[word_]\n",
    "                        else:\n",
    "                            self.embeddings_matrix[id]=np.random.rand(self.embeddings_model.vector_size)\n",
    "                    id+=1\n",
    "        #siempre agregar los tokens especiales  \n",
    "        self.w2id.update({self.UNK:id,self.SOS:id+1,self.EOS:id+2})\n",
    "        self.id2w.update({id:self.UNK,id+1:self.SOS,id+2:self.EOS})\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def transform(self,corpus:list)-> Tuple [np.array,np.array]:\n",
    "        X_ngrams=[]\n",
    "        y=[]\n",
    "        for doc in corpus:\n",
    "            doc_ngram=self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                #print(\"words_window:\", words_window)\n",
    "                words_window_ids = [self.w2id[w] if w in self.w2id else self.w2id[\"<unk>\"] for w in words_window]\n",
    "\n",
    "                #words_window_ids=[self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "        return np.array(X_ngrams),np.array(y)\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_ngram_doc(self, doc: str) -> list:\n",
    "        doc_tokens = self.tokenizer(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS] * (self.N - 1) + doc_tokens + [self.EOS]  # Corrección aquí\n",
    "        return list(ngrams(doc_tokens, self.N))\n",
    "    \n",
    "    def replace_unk(self,doc_tokens:list) -> list:\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab: #si el token no essta en minuscula entonces reemplazar por UNK\n",
    "                doc_tokens[i]=self.UNK\n",
    "        return doc_tokens #regresa tokens ya procesados\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk=TweetTokenizer()\n",
    "ngram_data=NgramData(args.N,5000,tk.tokenize)\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train,y_ngram_train=ngram_data.transform(X_train)\n",
    "X_ngram_val,y_ngram_val=ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations (X): (102751, 3),y: (102751,)\n",
      "Validation observations (X): (11558, 3),y: (11558,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training observations (X): {X_ngram_train.shape},y: {y_ngram_train.shape}')\n",
    "print(f'Validation observations (X): {X_ngram_val.shape},y: {y_ngram_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set batch size in args\n",
    "args.batch_size=64\n",
    "#Num workers \n",
    "args.num_workers=0\n",
    "\n",
    "#Train\n",
    "train_dataset=TensorDataset(torch.tensor(X_ngram_train,dtype=torch.int64),torch.tensor(y_ngram_train,dtype=torch.int64))\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=args.batch_size,shuffle=True,num_workers=args.num_workers)\n",
    "\n",
    "#Val\n",
    "val_dataset=TensorDataset(torch.tensor(X_ngram_val,dtype=torch.int64),torch.tensor(y_ngram_val,dtype=torch.int64))\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=args.batch_size,shuffle=False,num_workers=args.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocab size\n",
    "args.vocab_size=ngram_data.get_vocab_size()\n",
    "\n",
    "#dimension of word embeddings\n",
    "args.d=50\n",
    "args.d_h=100\n",
    "args.dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM2(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(NeuralLM2,self).__init__()\n",
    "        self.window_size=args.N-1\n",
    "        self.embedding_dim=args.d\n",
    "        \n",
    "        self.emb=nn.Embedding(args.vocab_size,args.d)\n",
    "        self.fc1=nn.Linear(args.d*(args.N-1),args.d_h)\n",
    "        self.drop1=nn.Dropout(p=args.dropout) #apagar neuronas, en este caso bajo para evitar el sobreajuste\n",
    "        self.fc2=nn.Linear(args.d_h,args.vocab_size,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.emb(x)\n",
    "        x=x.view(-1,self.window_size*self.embedding_dim)\n",
    "        h=F.relu(self.fc1(x))\n",
    "        #quitar algunos elemntos para que no se sobreentrene\n",
    "        h=self.drop1(h)\n",
    "        return self.fc2(h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self,args, pretrained_embeddings=None):\n",
    "        super(NeuralLM,self).__init__()\n",
    "        self.window_size=args.N-1\n",
    "        self.embedding_dim=args.d\n",
    "        \n",
    "        #self.emb = nn.Embedding.from_pretrained(\n",
    "        #    torch.tensor(pretrained_embeddings, dtype=torch.float32),\n",
    "        #    freeze=False  # Debe ser False para que los embeddings se actualicen\n",
    "        #    )\n",
    "\n",
    "        \n",
    "        # Si hay embeddings preentrenados, los usamos\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.emb = nn.Embedding.from_pretrained(\n",
    "            torch.tensor(pretrained_embeddings, dtype=torch.float32),\n",
    "            freeze=False  # Debe ser False para que los embeddings se actualicen\n",
    "            )\n",
    "        #    self.emb = nn.Embedding.from_pretrained(torch.tensor(pretrained_embeddings, dtype=torch.float32), freeze=False)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "        self.fc1=nn.Linear(args.d*(args.N-1),args.d_h)\n",
    "        #self.fc1=nn.Linear(args.d * self.window_size, args.d_h)\n",
    "        self.drop1=nn.Dropout(p=args.dropout)\n",
    "        self.fc2=nn.Linear(args.d_h, args.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.window_size * self.embedding_dim)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizar la red para obtener la probabilidad 1\n",
    "def get_preds(raw_logits):\n",
    "    probs=F.softmax(raw_logits.detach(),dim=1) #solamente a los valores de la matriz del tensor\n",
    "    y_pred=torch.argmax(probs,dim=1).cpu().numpy() #argmax para indice\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data,model,gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds,tgts=[],[]\n",
    "        for window_words,labels in data:\n",
    "            if gpu:\n",
    "                window_words=window_words.cuda()\n",
    "            outputs=model(window_words)\n",
    "            \n",
    "            #Get predictions\n",
    "            y_pred=get_preds(outputs)\n",
    "            tgt=labels.numpy()\n",
    "            tgt_list = tgt.tolist()  # Convertir a lista de Python\n",
    "            tgts.append(tgt_list)  # Ahora sí puedes hacer append()\n",
    "\n",
    "            #tgt.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "    tgts=[e for l in tgts for e in l]\n",
    "    preds=[e for l in preds for e in l]\n",
    "    \n",
    "    return accuracy_score(tgts,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state,is_best,checkpoint_path,filename='checkpoint.pt'):\n",
    "    filename=os.path.join(checkpoint_path,filename)\n",
    "    torch.save(state,filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename,os.path.join(checkpoint_path,'model_best.pt')) #shutil es una libreria que permite copiar archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likehood(model,text,ngram_model):\n",
    "    X,y=ngram_data.transform([text])\n",
    "    X,y=X[2:],y[2:]\n",
    "    X=torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    logits=model(X).detach()\n",
    "    probs=F.softmax(logits,dim=1).numpy()\n",
    "    \n",
    "    return (np.sum([np.log(probs[i][w]) for i,w in enumerate(y)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = lambda x: list(x)\n",
    "ngram_data_char = NgramData( args.N, 5000, tokenizer_char )\n",
    "ngram_data_char.fit( X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train,y_ngram_train=ngram_data_char.transform(X_train)\n",
    "X_ngram_val,y_ngram_val=ngram_data_char.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 64\n",
    "args.num_workers = 0\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype=torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype=torch.int64))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size= args.batch_size,\n",
    "                          num_workers= args.num_workers,\n",
    "                          shuffle= True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype=torch.int64),\n",
    "                            torch.tensor(y_ngram_val, dtype=torch.int64))\n",
    "\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size= args.batch_size,\n",
    "                          num_workers= args.num_workers,\n",
    "                          shuffle= True)\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vocab_size = ngram_data_char.get_vocab_size()\n",
    "args.d = 100 \n",
    "args.d_h = 200 \n",
    "args.dropout = 0.1\n",
    "\n",
    "# Training\n",
    "args.lr = 1e-3 \n",
    "args.num_epochs = 100\n",
    "args.patience = 20 \n",
    "\n",
    "# Scheduler\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "# Save\n",
    "args.save_dir = 'char_model'\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "args.use_gpu=torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_char = NeuralLM( args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model_char.parameters(),lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.25377462767607817\n",
      "Epochh [1/100], Loss: 3.1522 - Val accuracy:0.3211 - Epoch time:1742440606.36\n",
      "Train acc: 0.32609525286999685\n",
      "Epochh [2/100], Loss: 2.3803 - Val accuracy:0.3515 - Epoch time:1742440611.10\n",
      "Train acc: 0.34730666304685076\n",
      "Epochh [3/100], Loss: 2.2644 - Val accuracy:0.3661 - Epoch time:1742440615.51\n",
      "Train acc: 0.3630930034129693\n",
      "Epochh [4/100], Loss: 2.2007 - Val accuracy:0.3802 - Epoch time:1742440620.61\n",
      "Train acc: 0.37441203847347193\n",
      "Epochh [5/100], Loss: 2.1572 - Val accuracy:0.3922 - Epoch time:1742440625.25\n",
      "Train acc: 0.38338737201365186\n",
      "Epochh [6/100], Loss: 2.1243 - Val accuracy:0.3985 - Epoch time:1742440629.61\n",
      "Train acc: 0.39004111076636677\n",
      "Epochh [7/100], Loss: 2.0961 - Val accuracy:0.4019 - Epoch time:1742440634.43\n",
      "Train acc: 0.3953073611542042\n",
      "Epochh [8/100], Loss: 2.0724 - Val accuracy:0.4067 - Epoch time:1742440638.50\n",
      "Train acc: 0.40004401954700586\n",
      "Epochh [9/100], Loss: 2.0524 - Val accuracy:0.4118 - Epoch time:1742440642.90\n",
      "Train acc: 0.40469360843934227\n",
      "Epochh [10/100], Loss: 2.0342 - Val accuracy:0.4168 - Epoch time:1742440647.02\n",
      "Train acc: 0.40891056469128145\n",
      "Epochh [11/100], Loss: 2.0185 - Val accuracy:0.4205 - Epoch time:1742440651.47\n",
      "Train acc: 0.41295299410487124\n",
      "Epochh [12/100], Loss: 2.0035 - Val accuracy:0.4254 - Epoch time:1742440656.40\n",
      "Train acc: 0.41666828265591066\n",
      "Epochh [13/100], Loss: 1.9911 - Val accuracy:0.4290 - Epoch time:1742440660.89\n",
      "Train acc: 0.41871974868135275\n",
      "Epochh [14/100], Loss: 1.9796 - Val accuracy:0.4308 - Epoch time:1742440666.80\n",
      "Train acc: 0.42160894353087186\n",
      "Epochh [15/100], Loss: 1.9691 - Val accuracy:0.4333 - Epoch time:1742440671.52\n",
      "Train acc: 0.42531317871548247\n",
      "Epochh [16/100], Loss: 1.9579 - Val accuracy:0.4359 - Epoch time:1742440675.83\n",
      "Train acc: 0.42703789171579276\n",
      "Epochh [17/100], Loss: 1.9492 - Val accuracy:0.4386 - Epoch time:1742440680.09\n",
      "Train acc: 0.42884191746819733\n",
      "Epochh [18/100], Loss: 1.9406 - Val accuracy:0.4401 - Epoch time:1742440685.36\n",
      "Train acc: 0.4311293825628296\n",
      "Epochh [19/100], Loss: 1.9325 - Val accuracy:0.4436 - Epoch time:1742440690.00\n",
      "Train acc: 0.43303327645051193\n",
      "Epochh [20/100], Loss: 1.9255 - Val accuracy:0.4442 - Epoch time:1742440694.63\n",
      "Train acc: 0.4349129304995346\n",
      "Epochh [21/100], Loss: 1.9180 - Val accuracy:0.4467 - Epoch time:1742440699.31\n",
      "Train acc: 0.43721396990381634\n",
      "Epochh [22/100], Loss: 1.9117 - Val accuracy:0.4479 - Epoch time:1742440703.68\n",
      "Train acc: 0.4379349596649085\n",
      "Epochh [23/100], Loss: 1.9064 - Val accuracy:0.4516 - Epoch time:1742440707.99\n",
      "Train acc: 0.43974266987278937\n",
      "Epochh [24/100], Loss: 1.9007 - Val accuracy:0.4512 - Epoch time:1742440712.57\n",
      "Train acc: 0.4407510471610301\n",
      "Epochh [25/100], Loss: 1.8958 - Val accuracy:0.4528 - Epoch time:1742440717.38\n",
      "Train acc: 0.4422015591064226\n",
      "Epochh [26/100], Loss: 1.8905 - Val accuracy:0.4534 - Epoch time:1742440721.87\n",
      "Train acc: 0.44293088737201364\n",
      "Epochh [27/100], Loss: 1.8861 - Val accuracy:0.4552 - Epoch time:1742440726.29\n",
      "Train acc: 0.444820819112628\n",
      "Epochh [28/100], Loss: 1.8810 - Val accuracy:0.4567 - Epoch time:1742440731.66\n",
      "Train acc: 0.4459540800496432\n",
      "Epochh [29/100], Loss: 1.8768 - Val accuracy:0.4583 - Epoch time:1742440736.49\n",
      "Train acc: 0.446554452373565\n",
      "Epochh [30/100], Loss: 1.8732 - Val accuracy:0.4586 - Epoch time:1742440742.16\n",
      "Train acc: 0.44735902109835557\n",
      "Epochh [31/100], Loss: 1.8687 - Val accuracy:0.4588 - Epoch time:1742440747.11\n",
      "Train acc: 0.4487069500465405\n",
      "Epochh [32/100], Loss: 1.8645 - Val accuracy:0.4604 - Epoch time:1742440752.04\n",
      "Train acc: 0.44956736735960284\n",
      "Epochh [33/100], Loss: 1.8614 - Val accuracy:0.4628 - Epoch time:1742440757.75\n",
      "Train acc: 0.4504361231771641\n",
      "Epochh [34/100], Loss: 1.8578 - Val accuracy:0.4623 - Epoch time:1742440763.31\n",
      "Train acc: 0.45151237201365185\n",
      "Epochh [35/100], Loss: 1.8544 - Val accuracy:0.4642 - Epoch time:1742440768.91\n",
      "Train acc: 0.45252327024511324\n",
      "Epochh [36/100], Loss: 1.8507 - Val accuracy:0.4643 - Epoch time:1742440773.33\n",
      "Train acc: 0.45300360688799257\n",
      "Epochh [37/100], Loss: 1.8477 - Val accuracy:0.4653 - Epoch time:1742440778.14\n",
      "Train acc: 0.4542838582066398\n",
      "Epochh [38/100], Loss: 1.8445 - Val accuracy:0.4657 - Epoch time:1742440782.50\n",
      "Train acc: 0.4548438954390319\n",
      "Epochh [39/100], Loss: 1.8409 - Val accuracy:0.4666 - Epoch time:1742440786.52\n",
      "Train acc: 0.4550510006205399\n",
      "Epochh [40/100], Loss: 1.8385 - Val accuracy:0.4662 - Epoch time:1742440790.39\n",
      "Train acc: 0.456420066708036\n",
      "Epochh [41/100], Loss: 1.8351 - Val accuracy:0.4680 - Epoch time:1742440794.22\n",
      "Train acc: 0.45713349363946637\n",
      "Epochh [42/100], Loss: 1.8323 - Val accuracy:0.4703 - Epoch time:1742440798.04\n",
      "Train acc: 0.45758241545144274\n",
      "Epochh [43/100], Loss: 1.8300 - Val accuracy:0.4692 - Epoch time:1742440801.89\n",
      "Train acc: 0.4584666847657462\n",
      "Epochh [44/100], Loss: 1.8276 - Val accuracy:0.4706 - Epoch time:1742440805.80\n",
      "Train acc: 0.45892142413900094\n",
      "Epochh [45/100], Loss: 1.8257 - Val accuracy:0.4718 - Epoch time:1742440809.68\n",
      "Train acc: 0.45973316785603474\n",
      "Epochh [46/100], Loss: 1.8230 - Val accuracy:0.4712 - Epoch time:1742440813.46\n",
      "Train acc: 0.4601892646602544\n",
      "Epochh [47/100], Loss: 1.8201 - Val accuracy:0.4727 - Epoch time:1742440817.28\n",
      "Train acc: 0.46077858361774743\n",
      "Epochh [48/100], Loss: 1.8182 - Val accuracy:0.4736 - Epoch time:1742440821.19\n",
      "Train acc: 0.4612732702451132\n",
      "Epochh [49/100], Loss: 1.8158 - Val accuracy:0.4742 - Epoch time:1742440825.01\n",
      "Train acc: 0.4615511557555073\n",
      "Epochh [50/100], Loss: 1.8135 - Val accuracy:0.4748 - Epoch time:1742440828.83\n",
      "Train acc: 0.46215734564070743\n",
      "Epochh [51/100], Loss: 1.8116 - Val accuracy:0.4751 - Epoch time:1742440832.64\n",
      "Train acc: 0.46283509152963076\n",
      "Epochh [52/100], Loss: 1.8099 - Val accuracy:0.4765 - Epoch time:1742440838.63\n",
      "Train acc: 0.46410855569345333\n",
      "Epochh [53/100], Loss: 1.8077 - Val accuracy:0.4764 - Epoch time:1742440842.52\n",
      "Train acc: 0.46384191746819736\n",
      "Epochh [54/100], Loss: 1.8055 - Val accuracy:0.4765 - Epoch time:1742440846.31\n",
      "Train acc: 0.4643660797393732\n",
      "Epochh [55/100], Loss: 1.8042 - Val accuracy:0.4770 - Epoch time:1742440850.12\n",
      "Train acc: 0.4648376900403351\n",
      "Epochh [56/100], Loss: 1.8021 - Val accuracy:0.4778 - Epoch time:1742440853.90\n",
      "Train acc: 0.46534730840831523\n",
      "Epochh [57/100], Loss: 1.7992 - Val accuracy:0.4782 - Epoch time:1742440857.69\n",
      "Train acc: 0.4660884657151722\n",
      "Epochh [58/100], Loss: 1.7979 - Val accuracy:0.4788 - Epoch time:1742440861.51\n",
      "Train acc: 0.46738907849829353\n",
      "Epochh [59/100], Loss: 1.7958 - Val accuracy:0.4787 - Epoch time:1742440865.35\n",
      "Train acc: 0.46650694228979206\n",
      "Epochh [60/100], Loss: 1.7950 - Val accuracy:0.4799 - Epoch time:1742440869.19\n",
      "Train acc: 0.4676597890164443\n",
      "Epochh [61/100], Loss: 1.7929 - Val accuracy:0.4799 - Epoch time:1742440873.00\n",
      "Train acc: 0.4684110300961837\n",
      "Epochh [62/100], Loss: 1.7907 - Val accuracy:0.4805 - Epoch time:1742440876.89\n",
      "Train acc: 0.4682632640397145\n",
      "Epochh [63/100], Loss: 1.7897 - Val accuracy:0.4819 - Epoch time:1742440880.72\n",
      "Train acc: 0.4683355957182749\n",
      "Epochh [64/100], Loss: 1.7874 - Val accuracy:0.4811 - Epoch time:1742440884.53\n",
      "Train acc: 0.4688632485262178\n",
      "Epochh [65/100], Loss: 1.7871 - Val accuracy:0.4821 - Epoch time:1742440888.34\n",
      "Train acc: 0.4695307167235495\n",
      "Epochh [66/100], Loss: 1.7848 - Val accuracy:0.4832 - Epoch time:1742440892.23\n",
      "Train acc: 0.46936860068259384\n",
      "Epochh [67/100], Loss: 1.7836 - Val accuracy:0.4821 - Epoch time:1742440896.05\n",
      "Train acc: 0.4695330437480608\n",
      "Epochh [68/100], Loss: 1.7829 - Val accuracy:0.4821 - Epoch time:1742440899.91\n",
      "Train acc: 0.4705864101768539\n",
      "Epochh [69/100], Loss: 1.7814 - Val accuracy:0.4829 - Epoch time:1742440903.76\n",
      "Train acc: 0.47079816940738445\n",
      "Epochh [70/100], Loss: 1.7798 - Val accuracy:0.4831 - Epoch time:1742440907.57\n",
      "Train acc: 0.4710845873409866\n",
      "Epochh [71/100], Loss: 1.7775 - Val accuracy:0.4841 - Epoch time:1742440911.43\n",
      "Train acc: 0.4714351923673596\n",
      "Epochh [72/100], Loss: 1.7768 - Val accuracy:0.4835 - Epoch time:1742440915.25\n",
      "Train acc: 0.4712224635432827\n",
      "Epochh [73/100], Loss: 1.7759 - Val accuracy:0.4840 - Epoch time:1742440919.07\n",
      "Train acc: 0.4716806934533044\n",
      "Epochh [74/100], Loss: 1.7741 - Val accuracy:0.4842 - Epoch time:1742440922.89\n",
      "Train acc: 0.4718020865653118\n",
      "Epochh [75/100], Loss: 1.7729 - Val accuracy:0.4840 - Epoch time:1742440926.70\n",
      "Train acc: 0.4725983167856035\n",
      "Epochh [76/100], Loss: 1.7718 - Val accuracy:0.4846 - Epoch time:1742440931.46\n",
      "Train acc: 0.4724978668941979\n",
      "Epochh [77/100], Loss: 1.7703 - Val accuracy:0.4852 - Epoch time:1742440935.41\n",
      "Train acc: 0.4731907384424449\n",
      "Epochh [78/100], Loss: 1.7690 - Val accuracy:0.4851 - Epoch time:1742440939.42\n",
      "Train acc: 0.47290567793980764\n",
      "Epochh [79/100], Loss: 1.7682 - Val accuracy:0.4853 - Epoch time:1742440943.33\n",
      "Train acc: 0.47382465870307167\n",
      "Epochh [80/100], Loss: 1.7673 - Val accuracy:0.4862 - Epoch time:1742440947.24\n",
      "Train acc: 0.47371373720136517\n",
      "Epochh [81/100], Loss: 1.7669 - Val accuracy:0.4867 - Epoch time:1742440951.22\n",
      "Train acc: 0.4729789792119144\n",
      "Epochh [82/100], Loss: 1.7658 - Val accuracy:0.4860 - Epoch time:1742440955.11\n",
      "Train acc: 0.474337185851691\n",
      "Epochh [83/100], Loss: 1.7641 - Val accuracy:0.4860 - Epoch time:1742440959.04\n",
      "Train acc: 0.47449057555072915\n",
      "Epochh [84/100], Loss: 1.7631 - Val accuracy:0.4873 - Epoch time:1742440963.24\n",
      "Train acc: 0.4750764039714552\n",
      "Epochh [85/100], Loss: 1.7617 - Val accuracy:0.4864 - Epoch time:1742440967.34\n",
      "Train acc: 0.47480860223394356\n",
      "Epochh [86/100], Loss: 1.7606 - Val accuracy:0.4878 - Epoch time:1742440971.15\n",
      "Train acc: 0.4754475643810115\n",
      "Epochh [87/100], Loss: 1.7590 - Val accuracy:0.4875 - Epoch time:1742440975.28\n",
      "Train acc: 0.4755466568414521\n",
      "Epochh [88/100], Loss: 1.7591 - Val accuracy:0.4879 - Epoch time:1742440980.30\n",
      "Train acc: 0.47621179801427244\n",
      "Epochh [89/100], Loss: 1.7570 - Val accuracy:0.4886 - Epoch time:1742440984.10\n",
      "Train acc: 0.4760390164443066\n",
      "Epochh [90/100], Loss: 1.7571 - Val accuracy:0.4889 - Epoch time:1742440987.84\n",
      "Train acc: 0.4766599441514117\n",
      "Epochh [91/100], Loss: 1.7555 - Val accuracy:0.4875 - Epoch time:1742440991.62\n",
      "Train acc: 0.47649899162271175\n",
      "Epochh [92/100], Loss: 1.7555 - Val accuracy:0.4888 - Epoch time:1742440995.40\n",
      "Train acc: 0.47623254731616504\n",
      "Epochh [93/100], Loss: 1.7545 - Val accuracy:0.4883 - Epoch time:1742440999.21\n",
      "Train acc: 0.47653603009618367\n",
      "Epochh [94/100], Loss: 1.7529 - Val accuracy:0.4887 - Epoch time:1742441003.03\n",
      "Train acc: 0.4773272184300341\n",
      "Epochh [95/100], Loss: 1.7523 - Val accuracy:0.4895 - Epoch time:1742441006.83\n",
      "Train acc: 0.47752327024511326\n",
      "Epochh [96/100], Loss: 1.7509 - Val accuracy:0.4892 - Epoch time:1742441010.61\n",
      "Train acc: 0.47789229754886753\n",
      "Epochh [97/100], Loss: 1.7502 - Val accuracy:0.4892 - Epoch time:1742441014.39\n",
      "Train acc: 0.4778858982314614\n",
      "Epochh [98/100], Loss: 1.7491 - Val accuracy:0.4893 - Epoch time:1742441018.15\n",
      "Train acc: 0.4778680577102079\n",
      "Epochh [99/100], Loss: 1.7489 - Val accuracy:0.4901 - Epoch time:1742441021.89\n",
      "Train acc: 0.4782527924294136\n",
      "Epochh [100/100], Loss: 1.7470 - Val accuracy:0.4898 - Epoch time:1742441025.64\n",
      "--- 524.0402390956879 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "best_metric=0\n",
    "metric_history=[]\n",
    "train_metric_history=[]\n",
    "\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time=time.time()\n",
    "    loss_epoch=[]\n",
    "    training_metric=[]\n",
    "    model_char.train()\n",
    "    \n",
    "    for window_words,labels in train_loader:\n",
    "        \n",
    "        if args.use_gpu:\n",
    "            window_words=window_words.cuda()\n",
    "            labels=labels.cuda()\n",
    "            \n",
    "        outputs=model_char(window_words)\n",
    "        loss=criterion(outputs,labels)  \n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        y_pred=get_preds(outputs)\n",
    "        tgt=labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt,y_pred))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    mean_epoch_metric=np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    model_char.eval()\n",
    "    tuning_metric=model_eval(val_loader,model_char,gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    is_improvement=tuning_metric>best_metric\n",
    "    if is_improvement:\n",
    "        best_metric=tuning_metric\n",
    "        n_no_improve=0\n",
    "    else:\n",
    "        n_no_improve+=1\n",
    "    \n",
    "    \n",
    "    save_checkpoint({\n",
    "        \"epoch\":epoch+1,\n",
    "        \"state_dict\":model_char.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "    },is_improvement,args.save_dir)\n",
    "    \n",
    "    \n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop\")\n",
    "        break\n",
    "    print('Train acc: {}'.format(mean_epoch_metric))\n",
    "    print('Epochh [{}/{}], Loss: {:.4f} - Val accuracy:{:.4f} - Epoch time:{:.2f}'\n",
    "          .format(epoch+1,args.num_epochs,np.mean(loss_epoch),tuning_metric,(time.time() - epoch)))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_char_model = NeuralLM(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, tokenizer):\n",
    "    all_tokens = tokenizer(text) \n",
    "    all_tokens = [w.lower() if w in ngram_data_char.w2id else '<unk>' for w in all_tokens]  \n",
    "    tokens_ids = [ngram_data_char.w2id.get(w.lower(), ngram_data_char.w2id[\"<unk>\"]) for w in all_tokens]\n",
    "    return all_tokens, tokens_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['h', 'o', 'l', 'a', ' ', 'm', 'u', 'n', 'd', 'o', '<unk>']\n",
      "Token IDs: [25, 5, 11, 2, 6, 12, 0, 14, 10, 5, 341]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"hola mundo!\"\n",
    "tokens, ids = parse_text(test_text, tokenizer_char)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model,initial_text,tokenizer): \n",
    "    #se cambio para que detecte los caracteres\n",
    "    all_tokens,window_word_ids=parse_text(initial_text,tokenizer)\n",
    "    for i in range(100):\n",
    "        y_pred=predict_next_token(model,window_word_ids) \n",
    "        next_word=ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        \n",
    "        if next_word == '<\\s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    return \" \".join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_generated_text(text):\n",
    "    import re\n",
    "    return re.sub(r'[^a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!? ]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, initial_text, tokenizer, max_length=300):\n",
    "    model.eval()\n",
    "    all_tokens, window_word_ids = parse_text(initial_text, tokenizer)\n",
    "\n",
    "    while len(window_word_ids) < args.N - 1:\n",
    "        window_word_ids.insert(0, ngram_data_char.w2id[\"<unk>\"])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        y_pred = predict_next_token(model, window_word_ids)  \n",
    "        next_word = ngram_data_char.id2w.get(y_pred, \"<unk>\")  \n",
    "\n",
    "        all_tokens.append(next_word)\n",
    "        \n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0) \n",
    "            window_word_ids.append(y_pred)\n",
    "\n",
    "    generated_text= \"\".join(all_tokens) \n",
    "    return clean_generated_text(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado para 'hola':\n",
      "holaé úuxtjies\n",
      "\n",
      "Texto generado para 'como':\n",
      "comocqsúübxdyxy u nfunksevvéxyqówks\n",
      "\n",
      "Texto generado para 'estas':\n",
      "estas  uizpmmgpueiáhjgeunkunkywpfqjgóanipunk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\"hola\", \"como\", \"estas\"]\n",
    "\n",
    "for inpu in input_texts:\n",
    "    generated_text = generate_sentence(model_char, inpu, tokenizer_char)\n",
    "    print(f\"Texto generado para '{inpu}':\\n{generated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood de 'coca': -13.142048835754395\n",
      "Likelihood de 'cola': -11.808069229125977\n",
      "Likelihood de 'es': -8.629572868347168\n",
      "Likelihood de 'amor': -9.064281463623047\n",
      "Likelihood de 'cafe': -13.684392929077148\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"coca\",\n",
    "    \"cola\",\n",
    "    \"es\",\n",
    "    \"amor\",\n",
    "    \"cafe\"\n",
    "]\n",
    "for sentence in sentences:\n",
    "    likelihood = log_likehood(model_char, sentence, ngram_data_char)\n",
    "    print(f\"Likelihood de '{sentence}': {likelihood}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_permutations(model, word, ngram_data_char):\n",
    "    char_list = list(word)  \n",
    "    perms = [''.join(perm) for perm in permutations(char_list)]\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Las más probables\")\n",
    "    for p, t in sorted([(log_likehood(model, text, ngram_data_char), text) for text in perms], reverse=True)[:5]:\n",
    "        print(p, t)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Las menos probables\")\n",
    "    for p, t in sorted([(log_likehood(model, text, ngram_data_char), text) for text in perms], reverse=True)[-5:]:\n",
    "        print(p, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Las más probables\n",
      "-4.7533803 madre\n",
      "-9.330919 mdare\n",
      "-9.621315 mdera\n",
      "-10.010485 darme\n",
      "-10.522482 marde\n",
      "------------------------------\n",
      "Las menos probables\n",
      "-21.793102 aedmr\n",
      "-21.861979 rmdae\n",
      "-22.092697 edmra\n",
      "-23.233648 eadmr\n",
      "-23.982748 daemr\n"
     ]
    }
   ],
   "source": [
    "evaluate_permutations(model_char, \"madre\", ngram_data_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo a nivel caracter 5.550668298053085\n"
     ]
    }
   ],
   "source": [
    "perplexity_char = perplexity(model_char, val_loader, ngram_data_char, use_gpu=args.use_gpu)\n",
    "print(\"Modelo a nivel caracter\", perplexity_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

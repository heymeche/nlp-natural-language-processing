{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e24c672",
   "metadata": {},
   "source": [
    "# Chi squared and tcor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6e4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ This notebook assumes that corpus processing, tokenization and BoW construction was already performed on the notebook:\n",
    "# ðŸ‘‰ 'feature-extraction/bag_of_words.ipynb'\n",
    "\n",
    "#The variables used here (such as `BoW_tr`, `tr_txt`, `V1`, `dict_indices1`) were built there.\n",
    "#If you want to re-run the pipeline from scratch, check that file first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3c2a1",
   "metadata": {},
   "source": [
    "> ðŸ”— **Note:** The corpus loading, tokenization and construction of the Bag of Words is at\n",
    "> [`bag_of_words.ipynb`](./feature-extraction/bag_of_words.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453838e",
   "metadata": {},
   "source": [
    "## TCOR\n",
    "TCOR measures relationships between terms (co-occurrence), weighted by:\n",
    "- tf (frequency): 1 + log(freq)\n",
    "- itf (inverse of co-occurrence frequency): log(total_terms / (docs_with_term + 1))\n",
    "\n",
    "Then, it is made into a standard form using columns (cosine).\n",
    "This matrix organizes each word into a group based on how it is connected to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b65bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tcor(BoW):\n",
    "    docs, terms = BoW.shape\n",
    "    co_ocur_matrix = np.dot(BoW.T, BoW) #Matriz de co-ocurrencias\n",
    "    TCOR = np.zeros_like(co_ocur_matrix, dtype=float)\n",
    "    co_ocur_term = np.count_nonzero(co_ocur_matrix, axis=1)\n",
    "\n",
    "    for term in range(terms):\n",
    "        for i in range(terms):\n",
    "            freq = co_ocur_matrix[term, i]\n",
    "            if freq > 0:\n",
    "                tf = 1 + np.log(freq)\n",
    "                itf = np.log(terms / (co_ocur_term[term] + 1)) #Calculo tf-idf\n",
    "                TCOR[term, i] = tf * itf\n",
    "            else:\n",
    "                TCOR[term, i] = 0\n",
    "\n",
    "    for i in range(terms): #normalizacion por cos\n",
    "        norm_factor = np.sqrt(np.sum(TCOR[:, i] ** 2))\n",
    "        if norm_factor > 0:\n",
    "            TCOR[:, i] /= norm_factor\n",
    "\n",
    "    return TCOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81ccf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz TCOR calculada con dimensiones: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "TCOR_base = compute_tcor(BoW_tr)\n",
    "print(\"Matriz TCOR calculada con dimensiones:\", TCOR_base.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951d5fe",
   "metadata": {},
   "source": [
    "## Chi squared\n",
    "For each word, calculate its chi-square statistic. This statistic measures how relevant the word is for distinguishing between different categories.\n",
    "Compare the observed and expected frequencies in each class.\n",
    "The higher the value, the more it matches the characteristics of a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113e431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chiSquare(BoW, y):\n",
    "    chi2s = np.zeros(BoW.shape[1])\n",
    "    docs, terms = BoW.shape\n",
    "    for i in range(terms):\n",
    "        obs = np.zeros(len(np.unique(y)))\n",
    "        expt = np.zeros(len(np.unique(y)))\n",
    "        for idx, cls in enumerate(np.unique(y)):\n",
    "            obs[idx] = BoW[y == cls, i].sum()    \n",
    "        total_obs = obs.sum()\n",
    "        for idx, cls in enumerate(np.unique(y)):\n",
    "            cls_prob = (y == cls).sum() / docs\n",
    "            expt[idx] = total_obs * cls_prob\n",
    "        non_zero_expected = expt != 0\n",
    "        chi2s[i] = np.sum(((obs[non_zero_expected] - expt[non_zero_expected]) ** 2) /expt[non_zero_expected])\n",
    "    return chi2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad1eb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5000)\n"
     ]
    }
   ],
   "source": [
    "k=1000\n",
    "chi2s = chiSquare(BoW_tr, tr_y)\n",
    "best_idx = np.argsort(chi2s)[-k:]\n",
    "dict_indices_invertido = {v: k for k, v in dict_indices1.items()}\n",
    "target_words = [dict_indices_invertido[index] for index in best_idx]\n",
    "t_words = target_words\n",
    "target_matriz = np.array([TCOR_base[dict_indices1[word]] for word in t_words])\n",
    "print(target_matriz.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
